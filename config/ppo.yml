### DeepMind Ball In Cup ENV with different reward function
DeepMindBallInCup-v0:
  name: DeepMind_Ball_In_Cup + PPO
  algorithm: ppo
  algo_params:
    batch_size: 50
    learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 1.e+6
    policy: MlpPolicy
    policy_type: on_policy
    special_callback: DMbicCallback
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      vf: 256
  env_params:
    env_name: alr_envs:DeepMindBallInCup-v0
    num_envs: 1
    wrapper: VecNormalize
  eval_env:
    n_eval_episode: 10
    eval_freq: 500

DeepMindBallInCup-v1:
  name: DeepMind_Ball_In_Cup + PPO
  algorithm: ppo
  algo_params:
    batch_size: 50
    learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 1.e+6
    policy: MlpPolicy
    policy_type: on_policy
    special_callback: DMbicCallback
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      vf: 256
  env_params:
    env_name: alr_envs:DeepMindBallInCup-v1
    num_envs: 1
    wrapper: VecNormalize
  eval_env:
    n_eval_episode: 10
    eval_freq: 500

DeepMindBallInCup-v2:
  name: DeepMind_Ball_In_Cup + PPO
  algorithm: ppo
  algo_params:
    batch_size: 50
    learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 1.e+6
    policy: MlpPolicy
    policy_type: on_policy
    special_callback: DMbicCallback
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      vf: 256
  env_params:
    env_name: alr_envs:DeepMindBallInCup-v2
    num_envs: 1
    wrapper: VecNormalize
  eval_env:
    n_eval_episode: 10
    eval_freq: 500

DeepMindBallInCupDense-v0:
  name: DeepMind_Ball_In_Cup + PPO
  algorithm: ppo
  algo_params:
    batch_size: 50
    learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 1.e+6
    policy: MlpPolicy
    policy_type: on_policy
    special_callback: DMbicCallback
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      vf: 256
  env_params:
    env_name: alr_envs:DeepMindBallInCupDense-v0
    num_envs: 1
    wrapper: VecNormalize
  eval_env:
    n_eval_episode: 10
    eval_freq: 500

DeepMindBallInCupDense-v1:
  name: DeepMind_Ball_In_Cup + PPO
  algorithm: ppo
  algo_params:
    batch_size: 50
    learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 1.e+6
    policy: MlpPolicy
    policy_type: on_policy
    special_callback: DMbicCallback
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      vf: 256
  env_params:
    env_name: alr_envs:DeepMindBallInCupDense-v1
    num_envs: 1
    wrapper: VecNormalize
  eval_env:
    n_eval_episode: 10
    eval_freq: 500

DeepMindBallInCupDense-v2:
  name: DeepMind_Ball_In_Cup + PPO
  algorithm: ppo
  algo_params:
    batch_size: 50
    learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 1.e+6
    policy: MlpPolicy
    policy_type: on_policy
    special_callback: DMbicCallback
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      vf: 256
  env_params:
    env_name: alr_envs:DeepMindBallInCupDense-v2
    num_envs: 1
    wrapper: VecNormalize
  eval_env:
    n_eval_episode: 10
    eval_freq: 500


FetchReacher-v0:
  algorithm: ppo
  algo_params:
    batch_size: 200
    learning_rate: 0.0001
    n_steps:  1000
    total_timesteps: 1.e+6
    policy: MlpPolicy
    policy_type: on_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      vf: 256
  env_params:
    env_name: alr_envs:FetchReacher-v0
    num_envs: 8
    wrapper: None
  eval_env:
    n_eval_episode: 1
    eval_freq: 1000

FetchReacher-v1:
  algorithm: ppo
  algo_params:
    batch_size: 200
    learning_rate: 0.0001
    n_steps:  1000
    total_timesteps: 1.e+6
    policy: MlpPolicy
    policy_type: on_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      vf: 256
  env_params:
    env_name: alr_envs:FetchReacher-v1
    num_envs: 8
    wrapper: None
  eval_env:
    n_eval_episode: 1
    eval_freq: 1000



### ALR Reacher ENV with different goal
ALRReacherBalanceIP-v3:
  algorithm: ppo
  algo_params:
    batch_size: 200
    learning_rate: 0.0001
    n_steps:  1000
    total_timesteps: 2.e+6
    policy: MlpPolicy
    policy_type: on_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      vf: 256
  env_params:
    env_name: alr_envs:ALRReacherBalanceIP-v3
    num_envs: 8
    wrapper: None
  eval_env:
    n_eval_episode: 1
    eval_freq: 1000

ALRReacherBalanceIP-v4:
  algorithm: ppo
  algo_params:
    batch_size: 200
    learning_rate: 0.0001
    n_steps:  1000
    total_timesteps: 2.e+6
    policy: MlpPolicy
    policy_type: on_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      vf: 256
  env_params:
    env_name: alr_envs:ALRReacherBalanceIP-v4
    num_envs: 8
    wrapper: None
  eval_env:
    n_eval_episode: 1
    eval_freq: 1000

ALRReacherBalanceIP-v5:
  algorithm: ppo
  algo_params:
    batch_size: 200
    learning_rate: 0.0001
    n_steps:  1000
    total_timesteps: 8.e+6
    policy: MlpPolicy
    policy_type: on_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      vf: 256
  env_params:
    env_name: alr_envs:ALRReacherBalanceIP-v5
    num_envs: 8
    wrapper: None
  eval_env:
    n_eval_episode: 1
    eval_freq: 1000

ALRReacherBalanceIP-v6:
  algorithm: ppo
  algo_params:
    batch_size: 200
    learning_rate: 0.0001
    n_steps: 1000
    total_timesteps: 8.e+6
    policy: MlpPolicy
    policy_type: on_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      vf: 256
  env_params:
    env_name: alr_envs:ALRReacherBalanceIP-v6
    num_envs: 8
    wrapper: None
  eval_env:
    n_eval_episode: 1
    eval_freq: 1000


### other ENVs
ALRBallInACupSimpleDense-v0:
  name: Ball_In_A_Cup + PPO
  algorithm: ppo
  algo_params:
    batch_size: 50
    learning_rate: 0.0001
    n_steps: 400
    total_timesteps: 5.e+7
    special_policy: CustomActorCriticPolicy
    policy_type: on_policy
    special_callback: ALRBallInACupCallback
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      vf: 256
  env_params:
    env_name: alr_envs:ALRBallInACupSimpleDense-v0
    num_envs: 8
    wrapper: VecNormalize


HoleReacherDense-v0:
  name: HoleReacherDense + PPO
  algorithm: ppo
  algo_params:
    batch_size: 50
    learning_rate: 0.0001
    n_steps: 300
    total_timesteps: 1.e+6
    policy: MlpPolicy
    policy_type: on_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      vf: 256
  env_params:
    env_name: alr_envs:HoleReacherDense-v0
    num_envs: 1
    wrapper: VecNormalize
  eval_env:
    n_eval_episode: 10
    eval_freq: 500


DeepMindWalkerDense-v0:
  name: DeepMind_Walker + PPO
  algorithm: ppo
  algo_params:
    batch_size: 50
    learning_rate: 0.0001
    n_steps: 2000
    total_timesteps: 2.e+6
    policy: MlpPolicy
    policy_type: on_policy
  env_params:
    env_name: alr_envs:DeepMindWalkerDense-v0
    num_envs: 8
    wrapper: VecNormalize



dmcCheetahDense-v0:
  name: DeepMind_Walker + PPO
  algorithm: ppo
  algo_params:
    batch_size: 50
    learning_rate: 0.0001
    n_steps: 2000
    total_timesteps: 2.e+6
    policy: MlpPolicy
    policy_type: on_policy
  env_params:
    env_name: alr_envs:dmcCheetahDense-v0
    num_envs: 8
    wrapper: None
  eval_env:
    n_eval_episode: 10
    eval_freq: 500


Meta-v2:
  algorithm: ppo
  algo_params:
    batch_size: 200
    learning_rate: 0.0001
    n_steps: 200
    total_timesteps: 2.e+7
    policy: MlpPolicy
    policy_type: on_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      vf: 256
  env_params:
    env_name: alr_envs
    num_envs: 10
    wrapper: None
  eval_env:
    n_eval_episode: 10
    eval_freq: 2000


HopperXYJumpMiddle-v0:
  algorithm: ppo
  algo_params:
    batch_size: 250
    learning_rate: 0.0001
    n_steps: 250
    total_timesteps: 1.e+7
    policy: MlpPolicy
    policy_type: on_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      vf: 256
  env_params:
    env_name: alr_envs:HopperXYJumpMiddle-v0
    num_envs: 8
    wrapper: VecNormalize
  eval_env:
    n_eval_episode: 1
    eval_freq: 250

HopperXYJumpStep-v0:
  algorithm: ppo
  algo_params:
    batch_size: 250
    learning_rate: 0.0001
    n_steps: 250
    total_timesteps: 5.e+6
    policy: MlpPolicy
    policy_type: on_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      vf: 256
  env_params:
    env_name: alr_envs:HopperXYJumpStep-v0
    num_envs: 8
    wrapper: VecNormalize
  eval_env:
    n_eval_episode: 1
    eval_freq: 250
