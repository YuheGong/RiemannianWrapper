### DeepMind Ball In Cup ENV with different reward function
DeepMindBallInCup-v0:
  name: DeepMind_Ball_In_Cup + SAC
  algorithm: sac
  algo_params:
    batch_size: 50
    learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 1.e+6
    special_callback: DMbicCallback
    train_freq: 1
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:DeepMindBallInCup-v0
    num_envs: 1
    wrapper: VecNormalize
  eval_env:
    n_eval_episode: 10
    eval_freq: 500

DeepMindBallInCup-v1:
  name: DeepMind_Ball_In_Cup + SAC
  algorithm: sac
  algo_params:
    batch_size: 50
    learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 1.e+6
    special_callback: DMbicCallback
    train_freq: 1
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:DeepMindBallInCup-v1
    num_envs: 1
    wrapper: VecNormalize
  eval_env:
    n_eval_episode: 10
    eval_freq: 500

DeepMindBallInCup-v2:
  name: DeepMind_Ball_In_Cup + SAC
  algorithm: sac
  algo_params:
    batch_size: 50
    learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 1.e+6
    special_callback: DMbicCallback
    train_freq: 1
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:DeepMindBallInCup-v2
    num_envs: 1
    wrapper: VecNormalize
  eval_env:
    n_eval_episode: 10
    eval_freq: 500

DeepMindBallInCupDense-v0:
  name: DeepMind_Ball_In_Cup + SAC
  algorithm: sac
  algo_params:
    batch_size: 50
    learning_rate: 0.0001
    n_steps: 500
    total_timesteps: 1.e+6
    special_callback: DMbicCallback
    train_freq: 1
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:DeepMindBallInCupDense-v0
    num_envs: 1
    wrapper: VecNormalize
  eval_env:
    n_eval_episode: 10
    eval_freq: 500

DeepMindBallInCupDense-v1:
  name: DeepMind_Ball_In_Cup + SAC
  algorithm: sac
  algo_params:
    batch_size: 50
    learning_rate: 0.0001
    n_steps: 200
    total_timesteps: 1.e+6
    special_callback: DMbicCallback
    train_freq: 1
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:DeepMindBallInCupDense-v1
    num_envs: 1
    wrapper: VecNormalize
  eval_env:
    n_eval_episode: 10
    eval_freq: 500

DeepMindBallInCupDense-v2:
  name: DeepMind_Ball_In_Cup + SAC
  algorithm: sac
  algo_params:
    batch_size: 50
    learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 1.e+6
    special_callback: DMbicCallback
    train_freq: 1
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:DeepMindBallInCupDense-v2
    num_envs: 1
    wrapper: VecNormalize
  eval_env:
    n_eval_episode: 10
    eval_freq: 500



### ALR Reacher ENV with different goal
ALRReacherBalance-v0:
  algorithm: sac
  algo_params:
    batch_size: 200
    learning_rate: 0.0001
    n_steps:  20000
    total_timesteps: 5.e+6
    gradient_steps: 200
    train_freq: 200
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:ALRReacherBalance-v0
    num_envs: 1
    wrapper: VecNormalize
  eval_env:
    n_eval_episode: 10
    eval_freq: 1000

ALRReacherBalanceIP-v3:
  algorithm: td3
  algo_params:
    batch_size: 100
    learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 1.e+6
    gradient_steps: 100
    train_freq: 100
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:ALRReacherBalanceIP-v3
    num_envs: 1
    wrapper: None
  eval_env:
    n_eval_episode: 1
    eval_freq: 2250



ALRReacherBalanceIP-v4:
  algorithm: td3
  algo_params:
    batch_size: 100
    learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 2.e+6
    gradient_steps: 100
    train_freq: 100
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:ALRReacherBalanceIP-v4
    num_envs: 1
    wrapper: None
  eval_env:
    n_eval_episode: 1
    eval_freq: 2250

ALRReacherBalanceIP-v5:
  algorithm: td3
  algo_params:
    batch_size: 100
    learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 8.e+6
    gradient_steps: 100
    train_freq: 100
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:ALRReacherBalanceIP-v5
    num_envs: 1
    wrapper: None
  eval_env:
    n_eval_episode: 1
    eval_freq: 2250

ALRReacherBalanceIP-v6:
  algorithm: td3
  algo_params:
    batch_size: 100
    learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 1.e+6
    gradient_steps: 100
    train_freq: 100
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:ALRReacherBalanceIP-v6
    num_envs: 1
    wrapper: None
  eval_env:
    n_eval_episode: 1
    eval_freq: 2250

ALRReacherBalance-v2:
  algorithm: td3
  algo_params:
    batch_size: 200
    learning_rate: 0.0001
    n_steps:  20000
    total_timesteps: 1.e+6
    train_freq: 200
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:ALRReacherBalance-v2
    num_envs: 1
    wrapper: VecNormalize
  eval_env:
    n_eval_episode: 10
    eval_freq: 1000

ALRReacherBalanceSparse-v2:
  algorithm: td3
  algo_params:
    batch_size: 200
    learning_rate: 0.0001
    n_steps:  20000
    total_timesteps: 1.e+6
    train_freq: 200
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:ALRReacherBalanceSparse-v2
    num_envs: 1
    wrapper: VecNormalize
  eval_env:
    n_eval_episode: 10
    eval_freq: 1000

FetchReacher-v0:
  algorithm: td3
  algo_params:
    batch_size: 100
    learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 1.e+6
    gradient_steps: 100
    train_freq: 100
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:FetchReacher-v0
    num_envs: 1
    wrapper: None
  eval_env:
    n_eval_episode: 1
    eval_freq: 100

FetchReacher-v1:
  algorithm: td3
  algo_params:
    batch_size: 100
    learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 1.e+6
    gradient_steps: 100
    train_freq: 100
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:FetchReacher-v1
    num_envs: 1
    wrapper: None
  eval_env:
    n_eval_episode: 1
    eval_freq: 100

### Meta World
MetaButtonPress-v2:
  algorithm: td3
  algo_params:
    batch_size: 200
    learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 2.e+6
    train_freq: 1
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:MetaButtonPress-v2
    num_envs: 1
    wrapper: VecNormalize
  eval_env:
    n_eval_episode: 10
    eval_freq: 1000

MetaButtonPressSparse-v2:
  algorithm: sac
  algo_params:
    batch_size: 200
    learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 1.e+6
    train_freq: 1
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:MetaButtonPressSparse-v2
    num_envs: 1
    wrapper: VecNormalize
  eval_env:
    n_eval_episode: 10
    eval_freq: 1000




### other ENVs
ALRBallInACupSimpleDense-v0:
  name: Ball_In_A_Cup + SAC
  algorithm: sac
  algo_params:
    batch_size: 50
    learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 5.e+6
    special_callback: ALRBallInACupCallback
    special_policy: CustomActorCriticPolicy
    policy_type: off_policy
  env_params:
    env_name: alr_envs:ALRBallInACupSimpleDense-v0
    num_envs: 8
    wrapper: VecNormalize


HoleReacherDense-v0:
  name: HoleReacherDense + SAC
  algorithm: sac
  algo_params:
    batch_size: 50
    learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 1.e+6
    policy: MlpPolicy
    policy_type: off_policy
  env_params:
    env_name: alr_envs:HoleReacherDense-v0
    num_envs: 8
    wrapper: VecNormalize
  eval_env:
    n_eval_episode: 10
    eval_freq: 500


DeepMindWalkerDense-v0:
  name: DeepMind_Walker + PPO
  algorithm: ppo
  algo_params:
    batch_size: 50
    learning_rate: 0.0001
    n_steps: 2000
    total_timesteps: 2.e+6
    policy: MlpPolicy
    policy_type: off_policy
  env_params:
    env_name: alr_envs:DeepMindWalkerDense-v0
    num_envs: 8
    wrapper: VecNormalize

InvertedDoublePendulum-v0:
  name: DeepMind_Ball_In_Cup + SAC
  algorithm: sac
  algo_params:
    batch_size: 50
    learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 1.e+6
    special_callback: DMbicCallback
    train_freq: 1
    gradient_steps: 1
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:InvertedDoublePendulum-v0
    num_envs: 1
    wrapper: None
  eval_env:
    n_eval_episode: 10
    eval_freq: 500

dmcWalkerDense-v0:
  name: DeepMind_Walker + TD3
  algorithm: sac
  algo_params:
    batch_size: 200
    learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 2.e+6
    gradient_steps: 1
    train_freq: 1
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:dmcWalkerDense-v0
    num_envs: 1
    wrapper: None
  eval_env:
    n_eval_episode: 10
    eval_freq: 500

dmcCheetahDense-v0:
  name: DeepMind_Walker +  TD3
  algorithm: sac
  algo_params:
    batch_size: 200
    learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 2.e+6
    gradient_steps: 1
    train_freq: 1
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:dmcCheetahDense-v0
    num_envs: 1
    wrapper: None
  eval_env:
    n_eval_episode: 10
    eval_freq: 500


Meta-v2:
  name: DeepMind_Walker +  TD3
  algorithm: td3
  algo_params:
    batch_size: 200
    learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 2.e+7
    gradient_steps: 200
    train_freq: 200
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs
    num_envs: 1
    wrapper: None
  eval_env:
    n_eval_episode: 10
    eval_freq: 200


HopperXYJumpMiddle-v0:
  name: DeepMind_Walker +  TD3
  algorithm: td3
  algo_params:
    batch_size: 250
    learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 1.e+7
    gradient_steps: 250
    train_freq: 200
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:HopperXYJumpMiddle-v0
    num_envs: 1
    wrapper: VecNormalize
  eval_env:
    n_eval_episode: 1
    eval_freq: 250


HopperXYJumpStep-v0:
  name: DeepMind_Walker +  TD3
  algorithm: td3
  algo_params:
    batch_size: 250
    learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 5.e+6
    gradient_steps: 250
    train_freq: 250
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:HopperXYJumpStep-v0
    num_envs: 1
    wrapper: VecNormalize
  eval_env:
    n_eval_episode: 1
    eval_freq: 250
